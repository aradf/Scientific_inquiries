{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2942eabc",
   "metadata": {},
   "source": [
    "# üéì YOLO v5 Objects Detection: Label, Train and Test\n",
    "\n",
    "### &nbsp; &nbsp; üéõÔ∏è Section 3: Create custom dataset in YOLO format\n",
    "#### &nbsp; &nbsp; &nbsp; üî£ Lecture: Traffic Signs dataset in YOLO\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**Description:**  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*`Experiment` with Traffic Signs dataset. `Create` two versions of the dataset: with 4 classes and 43 classes.*  \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**File:** *`ts_dataset.ipynb`*  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10657b59",
   "metadata": {},
   "source": [
    "### üí° Algorithm:<a name=\"algorithm\"></a>\n",
    "\n",
    "**‚úîÔ∏è Step 1:** [Set up prerequisites](#step1)  \n",
    "**‚úîÔ∏è Step 2:** [Convert TS dataset in YOLO](#step2)  \n",
    "**‚úîÔ∏è Step 3:** [Load converted dataset from hard drive](#step3)  \n",
    "  \n",
    "  \n",
    "### üéØ **Results:**  \n",
    "**‚úÖ Converted TS dataset** with images and annotations **in YOLO format**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901ea62",
   "metadata": {},
   "source": [
    "<a name=\"step1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eccc72",
   "metadata": {},
   "source": [
    " ‚áß [Back to Algorithm](#algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d89c6b",
   "metadata": {},
   "source": [
    "# üì• Step 1: Set up prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da85c5",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üìú **Content:**  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 1.**1** **Load** needed libraries  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 1.**2** **Prepare** paths to directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f621b",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff6481",
   "metadata": {},
   "source": [
    "### üí† 1.1 Load needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6125dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/montecarlo/Desktop/scientific_computing/MachineLearningA-Z/fo-venv/lib/python3.9/site-packages (4.67.1)\n",
      "Installed tqdm üëå\n"
     ]
    }
   ],
   "source": [
    "# Installing 'tqdm' library to show smart progress bars\n",
    "# and track calculations inside loops in Real Time\n",
    "# Adding flag '-y' for silent installation\n",
    "# !conda install -c anaconda tqdm -y\n",
    "!pip3 install tqdm\n",
    "print('Installed tqdm \\U0001F44C')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries are successfully loaded üëå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo                 # To use all the FiftyOne functionality\n",
    "# import fiftyone.utils.splits as fous  # To split dataset into sub-datasets\n",
    "import fiftyone.utils.random as fous  # To split dataset into sub-datasets\n",
    "import os                             # To use operating system dependent functionality\n",
    "import glob                           # To find all the pathnames matching a specified pattern\n",
    "import pandas as pd                   # To load and process dataFrames\n",
    "import cv2                            # To load and process images\n",
    "from tqdm import tqdm                 # To track calculations inside loops in Real Time\n",
    "\n",
    "# Check point\n",
    "# Hint: to print emoji via Unicode, replace '+' with '000' and add prefix '\\'\n",
    "# For instance, emoji with Unicode 'U+1F44C' can be printed as '\\U0001F44C'\n",
    "print(\"Libraries are successfully loaded \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a3f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000000/10000000 [00:02<00:00, 4601216.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm installed ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifying successfull 'tqdm' installation\n",
    "for i in tqdm(range(10000000)):\n",
    "    ...\n",
    "print('tqdm installed ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7de298",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efd421",
   "metadata": {},
   "source": [
    "### üí† 1.2 Prepare paths to directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d674e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently active directory is:\n",
      "/home/montecarlo/Desktop/scientific_computing/yolov5_course/section3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check point\n",
    "# Showing currently active directory\n",
    "print('Currently active directory is:')\n",
    "print(os.getcwd())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43f9066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paths to directories are:\n",
      "/home/montecarlo/Desktop/scientific_computing/yolov5_course/section3/ts_original\n",
      "/home/montecarlo/Desktop/scientific_computing/yolov5_course/section3/ts_yolo/yolov5dataset/ts4classes\n",
      "/home/montecarlo/Desktop/scientific_computing/yolov5_course/section3/ts_yolo/yolov5dataset/ts43classes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preparing paths to directories\n",
    "directory_ts_original = os.path.join(os.getcwd(), \"ts_original\")\n",
    "directory_ts_yolo_4_classes = os.path.join(os.getcwd(), \"ts_yolo\", \"yolov5dataset\", \"ts4classes\")\n",
    "directory_ts_yolo_43_classes = os.path.join(os.getcwd(), \"ts_yolo\", \"yolov5dataset\", \"ts43classes\")\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('The paths to directories are:')\n",
    "print(directory_ts_original)\n",
    "print(directory_ts_yolo_4_classes)\n",
    "print(directory_ts_yolo_43_classes)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d07fe",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154bc2c",
   "metadata": {},
   "source": [
    "<a name=\"step2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3ca12",
   "metadata": {},
   "source": [
    " ‚áß [Back to Algorithm](#algorithm) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚áß [Back to Step 1 content](#step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3001ec9",
   "metadata": {},
   "source": [
    "# üñºÔ∏è Step 2: Convert TS dataset in YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d390d",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üìú **Content:**  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**1** **Prepare** groups of classes  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**2** **Load** original annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**3** **Calculate** FiftyOne format  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**4** **Convert** images to JPG format & **normalize** annotations  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**5** **Load** TS dataset from hard drive to FiftyOne  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;üí† 2.**6** **Convert** TS dataset in YOLO format for v5 version  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489b3c9",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ffe92",
   "metadata": {},
   "source": [
    "### üí† 2.1 Prepare groups of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62597d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The groups of classes are successfully prepared üëå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining lists of classes according to the classes ID's\n",
    "\n",
    "\n",
    "# Prohibitory group:\n",
    "# Circular traffic signs with white background and red border line\n",
    "prohibitory = [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 15, 16]\n",
    "\n",
    "\n",
    "# Danger group:\n",
    "# Triangular traffic signs with white background and red border line\n",
    "danger = [11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "\n",
    "\n",
    "# Mandatory group:\n",
    "# Circular traffic signs with blue background\n",
    "mandatory = [33, 34, 35, 36, 37, 38, 39, 40]\n",
    "\n",
    "\n",
    "# Other group:\n",
    "other = [6, 12, 13, 14, 17, 32, 41, 42]\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The groups of classes are successfully prepared \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7a97a",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106f876",
   "metadata": {},
   "source": [
    "### üí† 2.2 Load original annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b221c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annotations are successfully loaded üëå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading txt file with annotations separated by semicolon\n",
    "# Loading 6 columns into Pandas dataFrame\n",
    "# Giving at the same time names to the columns\n",
    "annotations = pd.read_csv(os.path.join(directory_ts_original, \"gt.txt\"),\n",
    "                          names=['ImageID',\n",
    "                                 'XMin',\n",
    "                                 'YMin',\n",
    "                                 'XMax',\n",
    "                                 'YMax',\n",
    "                                 'ClassID'],\n",
    "                          sep=';')\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The annotations are successfully loaded \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82e1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ImageID  XMin  YMin  XMax  YMax  ClassID\n",
      "0  00000.ppm   774   411   815   446       11\n",
      "1  00001.ppm   983   388  1024   432       40\n",
      "2  00001.ppm   386   494   442   552       38\n",
      "3  00001.ppm   973   335  1031   390       13\n",
      "4  00002.ppm   892   476  1006   592       39\n"
     ]
    }
   ],
   "source": [
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(annotations.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b0aef7",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e84e6",
   "metadata": {},
   "source": [
    "### üí† 2.3 Calculate FiftyOne format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06dd4b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The width and height are successfully calculated üëå\n",
      "The appropriate columns are successfully renamed üëå\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating bounding boxes' width and height for all rows at the same time\n",
    "annotations['XMax'] = annotations['XMax'] - annotations['XMin']  # width of the bounding box\n",
    "annotations['YMax'] = annotations['YMax'] - annotations['YMin']  # height of the bounding box\n",
    "\n",
    "\n",
    "# Renaming columns of the dataFrame inplace\n",
    "annotations.rename(columns={'XMax': 'Width', 'YMax': 'Height'}, inplace=True)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The width and height are successfully calculated \\U0001F44C\")\n",
    "print(\"The appropriate columns are successfully renamed \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44c54da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ImageID  XMin  YMin  Width  Height  ClassID\n",
      "0  00000.ppm   774   411     41      35       11\n",
      "1  00001.ppm   983   388     41      44       40\n",
      "2  00001.ppm   386   494     56      58       38\n",
      "3  00001.ppm   973   335     58      55       13\n",
      "4  00002.ppm   892   476    114     116       39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(annotations.head())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2aeda4",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2902a9d",
   "metadata": {},
   "source": [
    "### üí† 2.4 Convert images to JPG format & normalize annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40feeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing path pattern to find images\n",
    "images_patt = directory_ts_original + \"\\*.ppm\"\n",
    "\n",
    "# Iterating all the found images\n",
    "for filepath in tqdm(glob.glob(images_patt)):\n",
    "    # Reading current image and getting its real width and height\n",
    "    ppm_image = cv2.imread(filepath)\n",
    "    \n",
    "    # Slicing from tuple only first two elements\n",
    "    # We need real image height and width to normalize bounding boxes' coordinates\n",
    "    height_image, width_image = ppm_image.shape[:2]\n",
    "    \n",
    "    # Slicing only image filename and name\n",
    "    # filepath         --> C:\\Users\\valen\\yolov5course\\section3\\ts_original\\00000.ppm\n",
    "    # filapath[-9:]    --> 00000.ppm\n",
    "    # filapath[-9:-4]  --> 00000\n",
    "    filename_image = filepath[-9:]\n",
    "    name_image = filepath[-9:-4]\n",
    "    \n",
    "    # Preparing path where to save JPG image\n",
    "    path_to_jpg_image = os.path.join(directory_ts_original, name_image) + '.jpg'\n",
    "    \n",
    "    # Saving image in JPG format by OpenCV\n",
    "    # OpenCV uses extension to choose format to save image with\n",
    "    cv2.imwrite(path_to_jpg_image, ppm_image)\n",
    "    \n",
    "    # Getting sub-dataFrame\n",
    "    # Locating needed row(s) in the dataFrame for current image\n",
    "    # By using 'loc' method and condition 'annotations['ImageID'] == filename_image'\n",
    "    # we find row(s) with annotations for current image\n",
    "    # By using 'copy()' we create separate sub-dataFrame\n",
    "    rows_image = annotations.loc[annotations['ImageID'] == filename_image].copy()\n",
    "    \n",
    "    # Checking if there is no any annotations for current image\n",
    "    if rows_image.isnull().values.all():\n",
    "        # Skipping this image\n",
    "        continue\n",
    "    \n",
    "    # Normalizing bounding boxes' coordinates\n",
    "    # according to the real image width and height\n",
    "    rows_image[\"XMin\"] = rows_image[\"XMin\"] / width_image\n",
    "    rows_image[\"YMin\"] = rows_image[\"YMin\"] / height_image\n",
    "    rows_image[\"Width\"] = rows_image[\"Width\"] / width_image\n",
    "    rows_image[\"Height\"] = rows_image[\"Height\"] / height_image\n",
    "    \n",
    "    # Updating dataFrame with normalized values for current image\n",
    "    annotations.loc[annotations['ImageID'] == filename_image] = rows_image\n",
    "    \n",
    "print()\n",
    "# Check point\n",
    "print(\"Conversion and normalization are successfully done \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "935227b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ImageID  XMin  YMin  Width  Height  ClassID\n",
      "0  00000.ppm   774   411     41      35       11\n",
      "1  00001.ppm   983   388     41      44       40\n",
      "2  00001.ppm   386   494     56      58       38\n",
      "3  00001.ppm   973   335     58      55       13\n",
      "4  00002.ppm   892   476    114     116       39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(annotations.head())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94392c04",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefca08d",
   "metadata": {},
   "source": [
    "### üí† 2.**5** Load TS dataset from hard drive to FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53107718",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Loading dataset from hard drive (custom format):**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/index.html#custom-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702c3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing list of loaded datasets into FiftyOne\n",
    "print(fo.list_datasets())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting datasets from FiftyOne\n",
    "fo.load_dataset(\"ts-empty-classes-yolov5\").delete()\n",
    "fo.load_dataset(\"ts-4-classes-yolov5\").delete()\n",
    "fo.load_dataset(\"ts-43-classes-yolov5\").delete()\n",
    "\n",
    "fo.load_dataset(\"ts-4-classes-yolov5-copy\").delete()\n",
    "fo.load_dataset(\"ts-43-classes-yolov5-copy\").delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0225bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing path pattern to find images\n",
    "images_patt = directory_ts_original + \"\\*.jpg\"\n",
    "\n",
    "\n",
    "# Preparing lists to collect image samples in\n",
    "samples_ts_4 = []      # for image samples that have traffic signs\n",
    "samples_ts_43 = []     # for image samples that have traffic signs\n",
    "samples_ts_empty = []  # for image samples that does not have traffic signs\n",
    "# Images without traffic signs will be used by YOLO v5, when training\n",
    "\n",
    "\n",
    "# Iterating all the found images\n",
    "for filepath in glob.glob(images_patt):\n",
    "    \n",
    "    # Slicing only image name\n",
    "    # filepath         --> C:\\Users\\valen\\yolov5course\\section3\\ts_original\\00000.jpg\n",
    "    # filapath[-9:-4]  --> 00000\n",
    "    name_image = filepath[-9:-4]\n",
    "    \n",
    "    \n",
    "    # Getting sub-dataFrame\n",
    "    # Locating needed row(s) in the dataFrame for current image\n",
    "    # By using 'loc' method and condition 'annotations['ImageID'] == filename_image'\n",
    "    # we find row(s) with annotations for current image\n",
    "    # By using 'copy()' we create separate sub-dataFrame\n",
    "    rows_image = annotations.loc[annotations['ImageID'] == name_image + \".ppm\"].copy()\n",
    "    \n",
    "    \n",
    "    # Preparing lists to collect detections for current image in\n",
    "    detections_ts_4 = []      # for images that have traffic signs\n",
    "    detections_ts_43 = []     # for images that have traffic signs\n",
    "    detections_ts_empty = []  # for images that does not have traffic signs\n",
    "    \n",
    "    \n",
    "    # Checking if there is any annotation for current image\n",
    "    if rows_image.isnull().values.all():  # True, no annotations for current image\n",
    "        \n",
    "        # Preparing empty class\n",
    "        label = \"\"\n",
    "        \n",
    "        \n",
    "        # Preparing empty bounding box coordinates\n",
    "        bounding_box = []\n",
    "        \n",
    "        \n",
    "        # Adding empty annotation\n",
    "        detections_ts_empty.append(fo.Detection(label=label, bounding_box=bounding_box))\n",
    "        \n",
    "        \n",
    "        # Getting current image sample\n",
    "        sample_empty = fo.Sample(filepath=filepath)\n",
    "        \n",
    "        \n",
    "        # print(sample_empty)\n",
    "        #\n",
    "        # <Sample: {\n",
    "        #          'id': None,\n",
    "        #          'media_type': 'image',\n",
    "        #          'filepath': 'C:\\\\Users\\\\valen\\\\yolov5course\\\\section3\\\\ts_original\\\\00000.jpg',\n",
    "        #          'tags': [],\n",
    "        #          'metadata': None\n",
    "        #         }>\n",
    "        \n",
    "        \n",
    "        # Storing epmty detections in a field 'detections'\n",
    "        # Referencing all detections to current image sample\n",
    "        sample_empty[\"detections\"] = fo.Detections(detections=detections_ts_empty)\n",
    "        \n",
    "        \n",
    "        # Adding current image sample with empty detections to the list of samples\n",
    "        samples_ts_empty.append(sample_empty)\n",
    "        \n",
    "    \n",
    "    else:  # False, there is(are) annotation(s) for current image\n",
    "        \n",
    "        # Iterating all found objects (rows) for current image        \n",
    "        for index, row in rows_image.iterrows():\n",
    "            \n",
    "            # Preparing current class for TS dataset with 4 classes\n",
    "            if row[\"ClassID\"] in prohibitory:\n",
    "                label_4 = \"0\"        # [0, 1, 2, 3, 4, 5, 7, 8, 9, 10, 15, 16]\n",
    "            \n",
    "            elif row[\"ClassID\"] in danger:\n",
    "                label_4 = \"1\"        # [11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "            \n",
    "            elif row[\"ClassID\"] in mandatory:\n",
    "                label_4 = \"2\"        # [33, 34, 35, 36, 37, 38, 39, 40]\n",
    "            \n",
    "            elif row[\"ClassID\"] in other:\n",
    "                label_4 = \"3\"        # [6, 12, 13, 14, 17, 32, 41, 42]\n",
    "            \n",
    "            \n",
    "            # Preparing current class for TS dataset with 43 classes\n",
    "            # Converting the integer class id to the string by 'str'\n",
    "            label_43 = str(row[\"ClassID\"])\n",
    "            \n",
    "            \n",
    "            # Preparing current bounding box coordinates\n",
    "            # Bounding box coordinates should be relative values\n",
    "            # in [0, 1] in the following format:\n",
    "            # [top-left-x, top-left-y, width, height]\n",
    "            bounding_box = [row[\"XMin\"], row[\"YMin\"], row[\"Width\"], row[\"Height\"]]\n",
    "            \n",
    "            \n",
    "            # Adding current object annotation to the list of detections for current image\n",
    "            detections_ts_4.append(fo.Detection(label=label_4, bounding_box=bounding_box))\n",
    "            detections_ts_43.append(fo.Detection(label=label_43, bounding_box=bounding_box))\n",
    "            \n",
    "            \n",
    "            # print(detections_ts_43)\n",
    "            #\n",
    "            # [<Detection: {\n",
    "            #               'id': '62e4ffef6cafa3f389dd2f4d',\n",
    "            #               'attributes': BaseDict({}),\n",
    "            #               'tags': BaseList([]),\n",
    "            #               'label': '11',\n",
    "            #               'bounding_box': BaseList([0.569118, 0.51375, 0.030147, 0.04375]),\n",
    "            #               'mask': None,\n",
    "            #               'confidence': None,\n",
    "            #               'index': None,\n",
    "            #        }>]\n",
    "        \n",
    "        \n",
    "        # Getting current image sample\n",
    "        sample_4 = fo.Sample(filepath=filepath)\n",
    "        sample_43 = fo.Sample(filepath=filepath)\n",
    "        \n",
    "        \n",
    "        # Storing detections in a field 'detections'\n",
    "        # Referencing all detections to current image sample\n",
    "        sample_4[\"detections\"] = fo.Detections(detections=detections_ts_4)\n",
    "        sample_43[\"detections\"] = fo.Detections(detections=detections_ts_43)\n",
    "        \n",
    "        \n",
    "        # Adding current image sample with all its detections to the list of samples\n",
    "        samples_ts_4.append(sample_4)\n",
    "        samples_ts_43.append(sample_43)\n",
    "    \n",
    "\n",
    "# Creating datasets with empty detections\n",
    "dataset_ts_empty = fo.Dataset(\"ts-empty-classes-yolov5\")\n",
    "dataset_ts_empty.add_samples(samples_ts_empty)\n",
    "\n",
    "\n",
    "# Creating datasets with 4 classes\n",
    "dataset_ts_4 = fo.Dataset(\"ts-4-classes-yolov5\")\n",
    "dataset_ts_4.add_samples(samples_ts_4)\n",
    "\n",
    "\n",
    "# Creating datasets with 43 classes\n",
    "dataset_ts_43 = fo.Dataset(\"ts-43-classes-yolov5\")\n",
    "dataset_ts_43.add_samples(samples_ts_43)\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"Datasets are successfully created \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9af7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing list of loaded datasets into FiftyOne\n",
    "print(fo.list_datasets())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again datasets into FiftyOne\n",
    "dataset_ts_empty = fo.load_dataset(\"ts-empty-classes-yolov5\")\n",
    "dataset_ts_4_classes = fo.load_dataset(\"ts-4-classes-yolov5\")\n",
    "dataset_ts_43_classes = fo.load_dataset(\"ts-43-classes-yolov5\")\n",
    "\n",
    "\n",
    "# Making the datasets persistent\n",
    "dataset_ts_empty.persistent = True\n",
    "dataset_ts_4_classes.persistent = True\n",
    "dataset_ts_43_classes.persistent = True\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing dataset information\n",
    "print(dataset_ts_empty)\n",
    "print(\"---\")\n",
    "print(dataset_ts_4_classes)\n",
    "print(\"---\")\n",
    "print(dataset_ts_43_classes)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1ce7d",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Splitting dataset into sub-datasets:**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://voxel51.com/docs/fiftyone/api/fiftyone.utils.splits.html#fiftyone.utils.splits.random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01739d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting prepared datasets\n",
    "# It will also create tags for the images: train, validation and test\n",
    "fous.random_split(dataset_ts_empty, {\"train\": 1})\n",
    "fous.random_split(dataset_ts_4_classes, {\"train\": 0.7, \"validation\": 0.2, \"test\": 0.1})\n",
    "fous.random_split(dataset_ts_43_classes, {\"train\": 0.7, \"validation\": 0.2, \"test\": 0.1})\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing number of samples in sub-datasets\n",
    "print(dataset_ts_empty.count_sample_tags())      # {'train': 159}\n",
    "print(dataset_ts_4_classes.count_sample_tags())  # {'train': 519, 'validation': 148, 'test': 74}\n",
    "print(dataset_ts_4_classes.count_sample_tags())  # {'train': 519, 'validation': 148, 'test': 74}\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the samples together into the one dataset\n",
    "_ = dataset_ts_4_classes.merge_samples(dataset_ts_empty)\n",
    "_ = dataset_ts_43_classes.merge_samples(dataset_ts_empty)\n",
    "\n",
    "\n",
    "# Check points\n",
    "print(\"The datasets are successfully merged \\U0001F44C\")\n",
    "print()\n",
    "print(dataset_ts_4_classes.count_sample_tags())  # {'train': 678, 'validation': 148, 'test': 74}\n",
    "print(dataset_ts_4_classes.count_sample_tags())  # {'train': 678, 'validation': 148, 'test': 74}\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c99e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing datasets information\n",
    "print(dataset_ts_4_classes)\n",
    "print(\"---\")\n",
    "print(dataset_ts_43_classes)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d124cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_4_classes = fo.load_dataset(\"ts-4-classes-yolov5\")\n",
    "\n",
    "\n",
    "# Option 1\n",
    "# Launching the App 1st time, and visualizing the dataset\n",
    "# Creating session instance\n",
    "session = fo.launch_app(dataset_ts_4_classes)\n",
    "\n",
    "\n",
    "# Option 2\n",
    "# Updating the session\n",
    "# session.dataset = dataset_ts_4_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "session.clear_dataset()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The session is successfully cleared \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a37b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_43_classes = fo.load_dataset(\"ts-43-classes-yolov5\")\n",
    "\n",
    "\n",
    "# Option 1\n",
    "# Launching the App 1st time, and visualizing the dataset\n",
    "# Creating session instance\n",
    "# session = fo.launch_app(dataset_ts_43_classes)\n",
    "\n",
    "\n",
    "# Option 2\n",
    "# Updating the session\n",
    "session.dataset = dataset_ts_43_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cada33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "session.clear_dataset()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The session is successfully cleared \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c0cf7",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237913c0",
   "metadata": {},
   "source": [
    "### üí† 2.6 Convert TS dataset in YOLO format for v5 version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cf8b0c",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**YOLOv5Dataset:**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://voxel51.com/docs/fiftyone/user_guide/export_datasets.html#yolov5dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4bb40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing list of loaded datasets into FiftyOne\n",
    "print(fo.list_datasets())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e895e6",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Exporting full dataset for 4 classes in YOLO for v5 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a534f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_4_classes = fo.load_dataset(\"ts-4-classes-yolov5\")\n",
    "\n",
    "\n",
    "# The name of the sample field containing the label that you wish to export\n",
    "# Used when exporting labeled datasets (e.g., classification or detection)\n",
    "label_field = \"detections\"\n",
    "# label_field = \"ground_truth\"\n",
    "\n",
    "\n",
    "# The classes to be exported\n",
    "# All splits must use the same classes list\n",
    "classes = ['0', '1', '2', '3']\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of dataset to export\n",
    "# Any subclass of `fiftyone.types.Dataset` is supported\n",
    "dataset_type = fo.types.YOLOv5Dataset\n",
    "\n",
    "\n",
    "# Export dataset in YOLO format for v5 version\n",
    "for split in splits:    \n",
    "    \n",
    "    # Getting image samples for current split from the dataset by appropriate tag\n",
    "    split_view = dataset_ts_4_classes.match_tags(split)\n",
    "    \n",
    "    \n",
    "    # Exporting current split\n",
    "    split_view.export(\n",
    "                      export_dir=directory_ts_yolo_4_classes,\n",
    "                      dataset_type=dataset_type,\n",
    "                      label_field=label_field,\n",
    "                      split=split,\n",
    "                      classes=classes\n",
    "                     )\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fefdb8d",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Exporting full dataset for 43 classes in YOLO for v5 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_43_classes = fo.load_dataset(\"ts-43-classes-yolov5\")\n",
    "\n",
    "\n",
    "# The name of the sample field containing the label that you wish to export\n",
    "# Used when exporting labeled datasets (e.g., classification or detection)\n",
    "label_field = \"detections\"\n",
    "# label_field = \"ground_truth\"\n",
    "\n",
    "\n",
    "# The classes to be exported\n",
    "# All splits must use the same classes list\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', \n",
    "           '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', \n",
    "           '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', \n",
    "           '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42']\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of dataset to export\n",
    "# Any subclass of `fiftyone.types.Dataset` is supported\n",
    "dataset_type = fo.types.YOLOv5Dataset\n",
    "\n",
    "\n",
    "# Export dataset in YOLO format for v5 version\n",
    "for split in splits:    \n",
    "    \n",
    "    # Getting image samples for current split from the dataset by appropriate tag\n",
    "    split_view = dataset_ts_43_classes.match_tags(split)\n",
    "    \n",
    "    \n",
    "    # Exporting current split\n",
    "    split_view.export(\n",
    "                      export_dir=directory_ts_yolo_43_classes,\n",
    "                      dataset_type=dataset_type,\n",
    "                      label_field=label_field,\n",
    "                      split=split,\n",
    "                      classes=classes,\n",
    "                     )\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ba9e5",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Exporting full dataset for 4 classes in YOLO for v4 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Create additional directories inside 'ts_yolo' to keep TS dataset in YOLO for v4 version:\n",
    "#     one with name            --> 'yolov4dataset'\n",
    "#     and inside it one more       --> 'ts4classes'\n",
    "#     and inside it three more         --> 'train', 'validation', 'test'\n",
    "# \n",
    "#  2. Prepare paths variables to these directories:\n",
    "directory_ts_yolo_4_classes_train = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts4classes\", \"train\")\n",
    "\n",
    "directory_ts_yolo_4_classes_validation = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts4classes\", \"validation\")\n",
    "\n",
    "directory_ts_yolo_4_classes_test = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts4classes\", \"test\")\n",
    "\n",
    "\n",
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_4_classes = fo.load_dataset(\"ts-4-classes-yolov5\")\n",
    "\n",
    "\n",
    "# The name of the sample field containing the label that you wish to export\n",
    "# Used when exporting labeled datasets (e.g., classification or detection)\n",
    "label_field = \"detections\"\n",
    "# label_field = \"ground_truth\"\n",
    "\n",
    "\n",
    "# The classes to be exported\n",
    "# All splits must use the same classes list\n",
    "classes = ['0', '1', '2', '3']\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of dataset to export\n",
    "# Any subclass of `fiftyone.types.Dataset` is supported\n",
    "dataset_type = fo.types.YOLOv4Dataset\n",
    "\n",
    "\n",
    "# Export dataset in YOLO format for v4 version\n",
    "for split in splits:    \n",
    "    \n",
    "    # Getting image samples for current split from the dataset by appropriate tag\n",
    "    split_view = dataset_ts_4_classes.match_tags(split)\n",
    "    \n",
    "    \n",
    "    # Checking if it is 'train' split\n",
    "    if split == \"train\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_4_classes_train,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "        \n",
    "        \n",
    "    # Checking if it is 'validation' split\n",
    "    if split == \"validation\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_4_classes_validation,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "    \n",
    "    \n",
    "    # Checking if it is 'test' split\n",
    "    if split == \"test\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_4_classes_test,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "        \n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f5e1a",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Exporting full dataset for 43 classes in YOLO for v4 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe545ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Create additional directories inside 'ts_yolo' to keep TS dataset in YOLO for v4 version:\n",
    "#     one with name            --> 'yolov4dataset'\n",
    "#     and inside it one more       --> 'ts43classes'\n",
    "#     and inside it three more         --> 'train', 'validation', 'test'\n",
    "# \n",
    "#  2. Prepare paths variables to these directories:\n",
    "directory_ts_yolo_43_classes_train = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts43classes\", \"train\")\n",
    "\n",
    "directory_ts_yolo_43_classes_validation = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts43classes\", \"validation\")\n",
    "\n",
    "directory_ts_yolo_43_classes_test = \\\n",
    "            os.path.join(os.getcwd(), \"ts_yolo\", \"yolov4dataset\", \"ts43classes\", \"test\")\n",
    "\n",
    "\n",
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_43_classes = fo.load_dataset(\"ts-43-classes-yolov5\")\n",
    "\n",
    "\n",
    "# The name of the sample field containing the label that you wish to export\n",
    "# Used when exporting labeled datasets (e.g., classification or detection)\n",
    "label_field = \"detections\"\n",
    "# label_field = \"ground_truth\"\n",
    "\n",
    "\n",
    "# The classes to be exported\n",
    "# All splits must use the same classes list\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', \n",
    "           '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', \n",
    "           '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', \n",
    "           '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42']\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of dataset to export\n",
    "# Any subclass of `fiftyone.types.Dataset` is supported\n",
    "dataset_type = fo.types.YOLOv4Dataset\n",
    "\n",
    "\n",
    "# Export dataset in YOLO format for v4 version\n",
    "for split in splits:    \n",
    "    \n",
    "    # Getting image samples for current split from the dataset by appropriate tag\n",
    "    split_view = dataset_ts_43_classes.match_tags(split)\n",
    "    \n",
    "    \n",
    "    # Checking if it is 'train' split\n",
    "    if split == \"train\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_43_classes_train,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "        \n",
    "        \n",
    "    # Checking if it is 'validation' split\n",
    "    if split == \"validation\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_43_classes_validation,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "    \n",
    "    \n",
    "    # Checking if it is 'test' split\n",
    "    if split == \"test\":\n",
    "        # Exporting current split\n",
    "        split_view.export(\n",
    "                          export_dir=directory_ts_yolo_43_classes_test,\n",
    "                          dataset_type=dataset_type,\n",
    "                          label_field=label_field,\n",
    "                          classes=classes,\n",
    "                         )\n",
    "        \n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dd047",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e8d7d",
   "metadata": {},
   "source": [
    "<a name=\"step3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539f6a9",
   "metadata": {},
   "source": [
    " ‚áß [Back to Algorithm](#algorithm) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚áß [Back to Step 2 content](#step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97455307",
   "metadata": {},
   "source": [
    "# üì• Step 3: Load converted dataset from hard drive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fb734",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Loading dataset from hard drive:**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;https://voxel51.com/docs/fiftyone/user_guide/dataset_creation/datasets.html#loading-datasets-from-disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing list of loaded datasets into FiftyOne\n",
    "print(fo.list_datasets())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1e33cf",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Loading full dataset for 4 classes in YOLO for v5 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting datasets from FiftyOne\n",
    "fo.load_dataset(\"ts-4-classes-yolov5-copy\").delete()\n",
    "fo.load_dataset(\"ts-43-classes-yolov5-copy\").delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A name for the dataset\n",
    "name = \"ts-4-classes-yolov5-copy\"\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of the dataset being imported\n",
    "dataset_type = fo.types.YOLOv5Dataset\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_ts_4_classes_copy = fo.Dataset(name)\n",
    "\n",
    "\n",
    "# Using tags to mark the samples in each split\n",
    "for split in splits:\n",
    "    dataset_ts_4_classes_copy.add_dir(\n",
    "                                      dataset_dir=directory_ts_yolo_4_classes,\n",
    "                                      dataset_type=dataset_type,\n",
    "                                      split=split,\n",
    "                                      tags=split\n",
    "                                     )\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2eb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataset persistent\n",
    "dataset_ts_4_classes_copy.persistent = True\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing dataset information\n",
    "print(dataset_ts_4_classes_copy)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5938b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_4_classes_copy = fo.load_dataset(\"ts-4-classes-yolov5-copy\")\n",
    "\n",
    "\n",
    "# Option 1\n",
    "# Launching the App 1st time, and visualizing the dataset\n",
    "# Creating session instance\n",
    "# session = fo.launch_app(dataset_ts_4_classes_copy)\n",
    "\n",
    "\n",
    "# Option 2\n",
    "# Updating the session\n",
    "session.dataset = dataset_ts_4_classes_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49713a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "session.clear_dataset()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The session is successfully cleared \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d3ee65",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Loading full dataset for 43 classes in YOLO for v5 version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A name for the dataset\n",
    "name = \"ts-43-classes-yolov5-copy\"\n",
    "\n",
    "\n",
    "# The splits to export\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# The type of the dataset being imported\n",
    "dataset_type = fo.types.YOLOv5Dataset\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset_ts_43_classes_copy = fo.Dataset(name)\n",
    "\n",
    "\n",
    "# Using tags to mark the samples in each split\n",
    "for split in splits:\n",
    "    dataset_ts_43_classes_copy.add_dir(\n",
    "                                       dataset_dir=directory_ts_yolo_43_classes,\n",
    "                                       dataset_type=dataset_type,\n",
    "                                       split=split,\n",
    "                                       tags=split\n",
    "                                      )\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the dataset persistent\n",
    "dataset_ts_43_classes_copy.persistent = True\n",
    "\n",
    "\n",
    "# Check point\n",
    "# Showing dataset information\n",
    "print(dataset_ts_43_classes_copy)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a22654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading again dataset into FiftyOne\n",
    "dataset_ts_43_classes_copy = fo.load_dataset(\"ts-43-classes-yolov5-copy\")\n",
    "\n",
    "\n",
    "# Option 1\n",
    "# Launching the App 1st time, and visualizing the dataset\n",
    "# Creating session instance\n",
    "# session = fo.launch_app(dataset_ts_43_classes_copy)\n",
    "\n",
    "\n",
    "# Option 2\n",
    "# Updating the session\n",
    "session.dataset = dataset_ts_43_classes_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35a6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "session.clear_dataset()\n",
    "\n",
    "\n",
    "# Check point\n",
    "print(\"The session is successfully cleared \\U0001F44C\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check point\n",
    "# Showing list of loaded datasets into FiftyOne\n",
    "print(fo.list_datasets())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867124e7",
   "metadata": {},
   "source": [
    " ‚áß [Back to Algorithm](#algorithm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
